#!/usr/bin/env python3
"""
Milvus å›¾æ–‡æœç´¢å®Œæ•´ç¤ºä¾‹
æ”¯æŒæ–‡æœ¬æœç´¢å›¾åƒå’Œå›¾åƒæœç´¢æ–‡æœ¬çš„å¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿ
"""

import os
import numpy as np
from PIL import Image
import requests
from io import BytesIO
import torch
from transformers import CLIPProcessor, CLIPModel
from pymilvus import MilvusClient, DataType, CollectionSchema, FieldSchema
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MultimodalSearchSystem:
    def __init__(self, milvus_uri="http://localhost:19530"):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ€æœç´¢ç³»ç»Ÿ
        
        Args:
            milvus_uri: MilvusæœåŠ¡å™¨åœ°å€
        """
        self.client = MilvusClient(uri=milvus_uri)
        self.collection_name = "multimodal_search"
        
        # åˆå§‹åŒ–CLIPæ¨¡å‹ - ç”¨äºç”Ÿæˆå›¾åƒå’Œæ–‡æœ¬çš„å‘é‡è¡¨ç¤º
        logger.info("åŠ è½½ CLIP æ¨¡å‹...")
        self.model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model.to(self.device)
        
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
    def create_collection(self):
        """åˆ›å»ºå¤šæ¨¡æ€æœç´¢é›†åˆ"""
        # æ£€æŸ¥é›†åˆæ˜¯å¦å·²å­˜åœ¨
        if self.client.has_collection(self.collection_name):
            logger.info(f"é›†åˆ {self.collection_name} å·²å­˜åœ¨ï¼Œåˆ é™¤åé‡æ–°åˆ›å»º")
            self.client.drop_collection(self.collection_name)
        
        # å®šä¹‰é›†åˆschema
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="image_path", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="image_description", dtype=DataType.VARCHAR, max_length=1000),
            FieldSchema(name="image_vector", dtype=DataType.FLOAT_VECTOR, dim=512),  # CLIPå‘é‡ç»´åº¦
            FieldSchema(name="text_vector", dtype=DataType.FLOAT_VECTOR, dim=512),   # CLIPå‘é‡ç»´åº¦
        ]
        
        schema = CollectionSchema(
            fields=fields,
            description="å¤šæ¨¡æ€å›¾æ–‡æœç´¢é›†åˆ"
        )
        
        # åˆ›å»ºé›†åˆ
        self.client.create_collection(
            collection_name=self.collection_name,
            schema=schema
        )
        
        logger.info(f"é›†åˆ {self.collection_name} åˆ›å»ºæˆåŠŸ")
        
    def create_indexes(self):
        """ä¸ºå‘é‡å­—æ®µåˆ›å»ºç´¢å¼•"""
        # ä¸ºå›¾åƒå‘é‡åˆ›å»ºç´¢å¼•
        image_index_params = {
            "index_type": "HNSW",
            "metric_type": "COSINE",
            "params": {"M": 16, "efConstruction": 200}
        }
        
        self.client.create_index(
            collection_name=self.collection_name,
            field_name="image_vector",
            index_params=image_index_params
        )
        
        # ä¸ºæ–‡æœ¬å‘é‡åˆ›å»ºç´¢å¼•
        text_index_params = {
            "index_type": "HNSW", 
            "metric_type": "COSINE",
            "params": {"M": 16, "efConstruction": 200}
        }
        
        self.client.create_index(
            collection_name=self.collection_name,
            field_name="text_vector",
            index_params=text_index_params
        )
        
        logger.info("ç´¢å¼•åˆ›å»ºæˆåŠŸ")
        
    def encode_image(self, image_path_or_url):
        """
        å°†å›¾åƒç¼–ç ä¸ºå‘é‡
        
        Args:
            image_path_or_url: å›¾åƒè·¯å¾„æˆ–URL
            
        Returns:
            numpy.ndarray: å›¾åƒå‘é‡
        """
        try:
            # åŠ è½½å›¾åƒ
            if image_path_or_url.startswith(('http://', 'https://')):
                response = requests.get(image_path_or_url)
                image = Image.open(BytesIO(response.content)).convert('RGB')
            else:
                image = Image.open(image_path_or_url).convert('RGB')
            
            # å¤„ç†å›¾åƒå¹¶ç”Ÿæˆå‘é‡
            inputs = self.processor(images=image, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                image_features = self.model.get_image_features(**inputs)
                # å½’ä¸€åŒ–å‘é‡
                image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)
                
            return image_features.cpu().numpy().flatten()
            
        except Exception as e:
            logger.error(f"å›¾åƒç¼–ç å¤±è´¥ {image_path_or_url}: {e}")
            return None
    
    def encode_text(self, text):
        """
        å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            numpy.ndarray: æ–‡æœ¬å‘é‡
        """
        try:
            # å¤„ç†æ–‡æœ¬å¹¶ç”Ÿæˆå‘é‡
            inputs = self.processor(text=text, return_tensors="pt", padding=True, truncation=True).to(self.device)
            
            with torch.no_grad():
                text_features = self.model.get_text_features(**inputs)
                # å½’ä¸€åŒ–å‘é‡
                text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)
                
            return text_features.cpu().numpy().flatten()
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬ç¼–ç å¤±è´¥ {text}: {e}")
            return None
    
    def insert_data(self, data_list):
        """
        æ’å…¥å›¾åƒå’Œæ–‡æœ¬æ•°æ®
        
        Args:
            data_list: æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {'image_path': '', 'description': ''}
        """
        entities = []
        
        for data in data_list:
            image_path = data['image_path']
            description = data['description']
            
            # ç”Ÿæˆå›¾åƒå‘é‡
            image_vector = self.encode_image(image_path)
            if image_vector is None:
                continue
                
            # ç”Ÿæˆæ–‡æœ¬å‘é‡
            text_vector = self.encode_text(description)
            if text_vector is None:
                continue
            
            entities.append({
                "image_path": image_path,
                "image_description": description,
                "image_vector": image_vector.tolist(),
                "text_vector": text_vector.tolist()
            })
            
            logger.info(f"å¤„ç†å®Œæˆ: {image_path}")
        
        # æ‰¹é‡æ’å…¥æ•°æ®
        if entities:
            self.client.insert(
                collection_name=self.collection_name,
                data=entities
            )
            logger.info(f"æˆåŠŸæ’å…¥ {len(entities)} æ¡æ•°æ®")
            
            # åˆ·æ–°æ•°æ®ç¡®ä¿ç«‹å³å¯æœç´¢
            self.client.flush(self.collection_name)
        else:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯æ’å…¥")
    
    def load_collection(self):
        """åŠ è½½é›†åˆåˆ°å†…å­˜"""
        self.client.load_collection(self.collection_name)
        logger.info("é›†åˆå·²åŠ è½½åˆ°å†…å­˜")
    
    def search_by_text(self, query_text, top_k=5):
        """
        ä½¿ç”¨æ–‡æœ¬æœç´¢ç›¸ä¼¼å›¾åƒ
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            list: æœç´¢ç»“æœ
        """
        # å°†æŸ¥è¯¢æ–‡æœ¬ç¼–ç ä¸ºå‘é‡
        query_vector = self.encode_text(query_text)
        if query_vector is None:
            return []
        
        # æ‰§è¡Œå‘é‡æœç´¢ - ç”¨æ–‡æœ¬å‘é‡æœç´¢å›¾åƒå‘é‡
        search_params = {"metric_type": "COSINE", "params": {"ef": 100}}
        
        results = self.client.search(
            collection_name=self.collection_name,
            data=[query_vector.tolist()],
            anns_field="image_vector",  # æœç´¢å›¾åƒå‘é‡å­—æ®µ
            search_params=search_params,
            limit=top_k,
            output_fields=["image_path", "image_description"]
        )
        
        return results[0] if results else []
    
    def search_by_image(self, query_image_path, top_k=5):
        """
        ä½¿ç”¨å›¾åƒæœç´¢ç›¸ä¼¼æ–‡æœ¬/å›¾åƒ
        
        Args:
            query_image_path: æŸ¥è¯¢å›¾åƒè·¯å¾„
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            list: æœç´¢ç»“æœ
        """
        # å°†æŸ¥è¯¢å›¾åƒç¼–ç ä¸ºå‘é‡
        query_vector = self.encode_image(query_image_path)
        if query_vector is None:
            return []
        
        # æ‰§è¡Œå‘é‡æœç´¢
        search_params = {"metric_type": "COSINE", "params": {"ef": 100}}
        
        results = self.client.search(
            collection_name=self.collection_name,
            data=[query_vector.tolist()],
            anns_field="image_vector",  # æœç´¢å›¾åƒå‘é‡å­—æ®µ
            search_params=search_params,
            limit=top_k,
            output_fields=["image_path", "image_description"]
        )
        
        return results[0] if results else []


def main():
    """ä¸»å‡½æ•°æ¼”ç¤º"""
    # åˆå§‹åŒ–æœç´¢ç³»ç»Ÿ
    search_system = MultimodalSearchSystem()
    
    # åˆ›å»ºé›†åˆå’Œç´¢å¼•
    search_system.create_collection()
    search_system.create_indexes()
    
    # å‡†å¤‡ç¤ºä¾‹æ•°æ® - å¯ä»¥ä½¿ç”¨æœ¬åœ°å›¾ç‰‡æˆ–ç½‘ç»œå›¾ç‰‡
    sample_data = [
        {
            "image_path": "https://images.unsplash.com/photo-1518717758536-85ae29035b6d?w=400",
            "description": "ä¸€åªå¯çˆ±çš„æ©™è‰²å°çŒ«ååœ¨é˜³å…‰ä¸‹"
        },
        {
            "image_path": "https://images.unsplash.com/photo-1552053831-71594a27632d?w=400", 
            "description": "ä¸€åªé‡‘æ¯›çŠ¬åœ¨è‰åœ°ä¸Šå¥”è·‘"
        },
        {
            "image_path": "https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400",
            "description": "ç¾ä¸½çš„å±±è„‰å’Œæ¹–æ³Šé£æ™¯"
        },
        {
            "image_path": "https://images.unsplash.com/photo-1542831371-29b0f74f9713?w=400",
            "description": "ç°ä»£åŒ–çš„åŸå¸‚å»ºç­‘å’Œå¤©é™…çº¿"
        },
        {
            "image_path": "https://images.unsplash.com/photo-1476224203421-9ac39bcb3327?w=400",
            "description": "äº”é¢œå…­è‰²çš„èŠ±æœµåœ¨èŠ±å›­ä¸­ç››å¼€"
        }
    ]
    
    # æ’å…¥æ•°æ®
    logger.info("å¼€å§‹æ’å…¥ç¤ºä¾‹æ•°æ®...")
    search_system.insert_data(sample_data)
    
    # åŠ è½½é›†åˆ
    search_system.load_collection()
    
    # æ¼”ç¤ºæ–‡æœ¬æœç´¢å›¾åƒ
    print("\n" + "="*50)
    print("ğŸ“ æ–‡æœ¬æœç´¢å›¾åƒç¤ºä¾‹")
    print("="*50)
    
    text_queries = [
        "å¯çˆ±çš„å°åŠ¨ç‰©",
        "è‡ªç„¶é£æ™¯", 
        "åŸå¸‚å»ºç­‘",
        "colorful flowers"
    ]
    
    for query in text_queries:
        print(f"\nğŸ” æŸ¥è¯¢: '{query}'")
        results = search_system.search_by_text(query, top_k=3)
        
        for i, result in enumerate(results, 1):
            print(f"  {i}. ç›¸ä¼¼åº¦: {1-result.distance:.3f}")
            print(f"     æè¿°: {result.entity.get('image_description')}")
            print(f"     å›¾ç‰‡: {result.entity.get('image_path')}")
    
    # æ¼”ç¤ºå›¾åƒæœç´¢
    print("\n" + "="*50)  
    print("ğŸ–¼ï¸ å›¾åƒæœç´¢ç¤ºä¾‹")
    print("="*50)
    
    # ä½¿ç”¨ç¬¬ä¸€å¼ å›¾ç‰‡ä½œä¸ºæŸ¥è¯¢
    query_image = sample_data[0]["image_path"]
    print(f"\nğŸ” ä½¿ç”¨å›¾åƒæŸ¥è¯¢: {query_image}")
    
    results = search_system.search_by_image(query_image, top_k=3)
    for i, result in enumerate(results, 1):
        print(f"  {i}. ç›¸ä¼¼åº¦: {1-result.distance:.3f}")
        print(f"     æè¿°: {result.entity.get('image_description')}")
        print(f"     å›¾ç‰‡: {result.entity.get('image_path')}")

if __name__ == "__main__":
    main() 