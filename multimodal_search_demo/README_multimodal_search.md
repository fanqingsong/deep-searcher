# Milvus å›¾æ–‡æœç´¢å®Œæ•´æ•™ç¨‹

æœ¬æ•™ç¨‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Milvus æ„å»ºå¤šæ¨¡æ€å‘é‡æ•°æ®åº“ï¼Œå®ç°å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„è·¨æ¨¡æ€æœç´¢ã€‚

## ğŸ¯ åŠŸèƒ½ç‰¹ç‚¹

- **æ–‡æœ¬æœç´¢å›¾åƒ**: è¾“å…¥æ–‡æœ¬æè¿°ï¼Œæ‰¾åˆ°ç›¸å…³å›¾ç‰‡
- **å›¾åƒæœç´¢æ–‡æœ¬**: è¾“å…¥å›¾ç‰‡ï¼Œæ‰¾åˆ°ç›¸å…³æ–‡æœ¬æè¿°  
- **å›¾åƒæœç´¢å›¾åƒ**: æ‰¾åˆ°è§†è§‰ä¸Šç›¸ä¼¼çš„å›¾ç‰‡
- **æ··åˆæœç´¢**: åŒæ—¶ä½¿ç”¨æ–‡æœ¬å’Œå›¾åƒè¿›è¡Œæœç´¢

## ğŸ“‹ ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–åŒ…

```bash
pip install -r requirements.txt
```

### 2. å¯åŠ¨ Milvus æœåŠ¡

**æ–¹å¼ä¸€: ä½¿ç”¨ Docker Compose (æ¨è)**
```bash
# ä½¿ç”¨é¡¹ç›®ä¸­çš„ docker-compose.yml
docker-compose up -d
```

**æ–¹å¼äºŒ: ä½¿ç”¨ Milvus Lite (æœ¬åœ°æ–‡ä»¶æ•°æ®åº“)**
```bash
# æ— éœ€é¢å¤–å®‰è£…ï¼Œç›´æ¥ä½¿ç”¨æ–‡ä»¶æ•°æ®åº“
# é€‚åˆå¼€å‘å’Œæµ‹è¯•
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç®€å•ç¤ºä¾‹

```bash
python simple_example.py
```

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºåŸºæœ¬çš„å‘é‡æœç´¢åŸç†ã€‚

### 2. å®Œæ•´å¤šæ¨¡æ€ç¤ºä¾‹

```bash
python multimodal_search_example.py
```

è¿™ä¸ªç¤ºä¾‹ä½¿ç”¨ CLIP æ¨¡å‹å®ç°çœŸæ­£çš„å›¾æ–‡æœç´¢ã€‚

## ğŸ”§ æ ¸å¿ƒæŠ€æœ¯åŸç†

### 1. å¤šæ¨¡æ€å‘é‡ç”Ÿæˆ

```python
from transformers import CLIPProcessor, CLIPModel

# åŠ è½½ CLIP æ¨¡å‹ 
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# å›¾åƒå‘é‡åŒ–
def encode_image(image):
    inputs = processor(images=image, return_tensors="pt")
    image_features = model.get_image_features(**inputs)
    return image_features.cpu().numpy()

# æ–‡æœ¬å‘é‡åŒ–  
def encode_text(text):
    inputs = processor(text=text, return_tensors="pt")
    text_features = model.get_text_features(**inputs)
    return text_features.cpu().numpy()
```

### 2. å‘é‡æ•°æ®åº“æ„å»º

```python
from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType

# å®šä¹‰æ•°æ®åº“ç»“æ„
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="image_path", dtype=DataType.VARCHAR, max_length=500),
    FieldSchema(name="description", dtype=DataType.VARCHAR, max_length=1000),
    FieldSchema(name="image_vector", dtype=DataType.FLOAT_VECTOR, dim=512),
    FieldSchema(name="text_vector", dtype=DataType.FLOAT_VECTOR, dim=512),
]

schema = CollectionSchema(fields=fields, description="å¤šæ¨¡æ€æœç´¢é›†åˆ")

# åˆ›å»ºé›†åˆ
client.create_collection(collection_name="multimodal_search", schema=schema)
```

### 3. åˆ›å»ºå‘é‡ç´¢å¼•

```python
# ä¸ºå›¾åƒå‘é‡åˆ›å»º HNSW ç´¢å¼•
image_index_params = {
    "index_type": "HNSW",
    "metric_type": "COSINE", 
    "params": {"M": 16, "efConstruction": 200}
}

client.create_index(
    collection_name="multimodal_search",
    field_name="image_vector",
    index_params=image_index_params
)
```

### 4. æ•°æ®æ’å…¥

```python
# å‡†å¤‡æ•°æ®
data = []
for image_path, description in image_text_pairs:
    # ç”Ÿæˆå‘é‡
    image_vector = encode_image(load_image(image_path))
    text_vector = encode_text(description)
    
    data.append({
        "image_path": image_path,
        "description": description,
        "image_vector": image_vector.tolist(),
        "text_vector": text_vector.tolist()
    })

# æ‰¹é‡æ’å…¥
client.insert("multimodal_search", data)
```

### 5. è·¨æ¨¡æ€æœç´¢

```python
# æ–‡æœ¬æœç´¢å›¾åƒ
def search_images_by_text(query_text, top_k=5):
    # å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
    query_vector = encode_text(query_text)
    
    # æœç´¢ç›¸ä¼¼çš„å›¾åƒå‘é‡
    results = client.search(
        collection_name="multimodal_search",
        data=[query_vector.tolist()],
        anns_field="image_vector",  # æœç´¢å›¾åƒå‘é‡å­—æ®µ
        limit=top_k,
        output_fields=["image_path", "description"]
    )
    return results

# å›¾åƒæœç´¢æ–‡æœ¬
def search_texts_by_image(query_image_path, top_k=5):
    # å°†å›¾åƒè½¬æ¢ä¸ºå‘é‡
    query_vector = encode_image(load_image(query_image_path))
    
    # æœç´¢ç›¸ä¼¼çš„æ–‡æœ¬å‘é‡
    results = client.search(
        collection_name="multimodal_search", 
        data=[query_vector.tolist()],
        anns_field="text_vector",  # æœç´¢æ–‡æœ¬å‘é‡å­—æ®µ
        limit=top_k,
        output_fields=["image_path", "description"]
    )
    return results
```

## ğŸ“Š å®é™…åº”ç”¨ç¤ºä¾‹

### 1. ç”µå•†å›¾ç‰‡æœç´¢

```python
# ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡ï¼Œæœç´¢ç›¸ä¼¼å•†å“
results = search_images_by_image("user_upload.jpg", top_k=10)

# ç”¨æˆ·è¾“å…¥æè¿°ï¼Œæœç´¢ç›¸å…³å•†å“
results = search_images_by_text("çº¢è‰²è¿è¡£è£™", top_k=10)
```

### 2. å†…å®¹ç®¡ç†ç³»ç»Ÿ

```python
# æ ¹æ®æ–‡ç« å†…å®¹æœç´¢ç›¸å…³å›¾ç‰‡
results = search_images_by_text("ç§‘æŠ€åˆ›æ–°å‘å±•", top_k=5)

# æ ¹æ®å›¾ç‰‡æœç´¢ç›¸å…³æ–‡ç« 
results = search_texts_by_image("tech_image.jpg", top_k=5)
```

### 3. æ™ºèƒ½ç›¸å†Œ

```python
# è¯­ä¹‰æœç´¢ç…§ç‰‡
results = search_images_by_text("ç”Ÿæ—¥èšä¼š", top_k=20)
results = search_images_by_text("æµ·è¾¹é£æ™¯", top_k=20)
```

## ğŸ›ï¸ é«˜çº§é…ç½®

### 1. ç´¢å¼•å‚æ•°è°ƒä¼˜

```python
# HNSW ç´¢å¼•å‚æ•°
index_params = {
    "index_type": "HNSW",
    "metric_type": "COSINE",
    "params": {
        "M": 16,              # è¿æ¥æ•°ï¼Œå½±å“ç´¢å¼•è´¨é‡å’Œå¤§å°
        "efConstruction": 200  # æ„å»ºæ—¶æœç´¢èŒƒå›´ï¼Œå½±å“ç´¢å¼•è´¨é‡
    }
}

# æœç´¢å‚æ•°  
search_params = {
    "metric_type": "COSINE",
    "params": {
        "ef": 100  # æœç´¢æ—¶èŒƒå›´ï¼Œå½±å“å¬å›ç‡å’Œæ€§èƒ½
    }
}
```

### 2. æ‰¹é‡å¤„ç†ä¼˜åŒ–

```python
# æ‰¹é‡æ’å…¥æ•°æ®
batch_size = 1000
for i in range(0, len(data), batch_size):
    batch = data[i:i + batch_size]
    client.insert("multimodal_search", batch)
    
    # å®šæœŸåˆ·æ–°ç¡®ä¿æ•°æ®å¯æœç´¢
    if i % (batch_size * 10) == 0:
        client.flush("multimodal_search")
```

### 3. å†…å­˜ç®¡ç†

```python
# åŠ è½½é›†åˆåˆ°å†…å­˜
client.load_collection("multimodal_search")

# é‡Šæ”¾é›†åˆå†…å­˜
client.release_collection("multimodal_search")
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. ç¡¬ä»¶é…ç½®

- **CPU**: 16+ æ ¸å¿ƒï¼Œæ”¯æŒ AVX2 æŒ‡ä»¤é›†
- **å†…å­˜**: 32GB+ (å­˜å‚¨å‘é‡æ•°æ®)
- **GPU**: NVIDIA GPU (åŠ é€Ÿå‘é‡è®¡ç®—)
- **å­˜å‚¨**: SSD (æå‡ I/O æ€§èƒ½)

### 2. å‚æ•°è°ƒä¼˜

```python
# ç´¢å¼•å‚æ•°æƒè¡¡
# M è¶Šå¤§ â†’ è´¨é‡è¶Šå¥½ï¼Œå†…å­˜å ç”¨è¶Šå¤§
# efConstruction è¶Šå¤§ â†’ è´¨é‡è¶Šå¥½ï¼Œæ„å»ºæ—¶é—´è¶Šé•¿
# ef è¶Šå¤§ â†’ å¬å›ç‡è¶Šé«˜ï¼Œæœç´¢æ—¶é—´è¶Šé•¿

# æ¨èé…ç½®
small_dataset = {"M": 8, "efConstruction": 100}   # < 100ä¸‡å‘é‡
medium_dataset = {"M": 16, "efConstruction": 200} # 100ä¸‡-1000ä¸‡å‘é‡  
large_dataset = {"M": 32, "efConstruction": 400}  # > 1000ä¸‡å‘é‡
```

### 3. å‘é‡ç»´åº¦é€‰æ‹©

```python
# ç»´åº¦ä¸æ€§èƒ½æƒè¡¡
dimensions = {
    384: "å¹³è¡¡æ€§èƒ½å’Œæ•ˆæœ",    # CLIP ViT-B/32
    512: "è¾ƒå¥½æ•ˆæœ",         # CLIP ViT-B/16  
    768: "æ›´å¥½æ•ˆæœ",         # CLIP ViT-L/14
    1024: "æœ€ä½³æ•ˆæœ"         # å¤§å‹æ¨¡å‹
}
```

## ğŸ” æ•…éšœæ’é™¤

### 1. å¸¸è§é”™è¯¯

```bash
# è¿æ¥å¤±è´¥
Error: failed to connect to Milvus
è§£å†³: æ£€æŸ¥ Milvus æœåŠ¡æ˜¯å¦å¯åŠ¨ï¼Œç«¯å£æ˜¯å¦æ­£ç¡®

# å†…å­˜ä¸è¶³
Error: insufficient memory  
è§£å†³: å¢åŠ å†…å­˜æˆ–å‡å°‘åŠ è½½çš„æ•°æ®é‡

# ç»´åº¦ä¸åŒ¹é…
Error: dimension mismatch
è§£å†³: ç¡®ä¿æŸ¥è¯¢å‘é‡ç»´åº¦ä¸é›†åˆå®šä¹‰ä¸€è‡´
```

### 2. æ€§èƒ½é—®é¢˜

```python
# æœç´¢æ…¢
# è§£å†³æ–¹æ¡ˆ:
1. æ£€æŸ¥æ˜¯å¦åˆ›å»ºäº†ç´¢å¼•
2. è°ƒæ•´æœç´¢å‚æ•° ef å€¼
3. ç¡®ä¿é›†åˆå·²åŠ è½½åˆ°å†…å­˜

# æ’å…¥æ…¢  
# è§£å†³æ–¹æ¡ˆ:
1. ä½¿ç”¨æ‰¹é‡æ’å…¥
2. å‡å°‘å‘é‡ç»´åº¦
3. ä¼˜åŒ–ç½‘ç»œè¿æ¥
```

## ğŸ“š æ‰©å±•é˜…è¯»

- [Milvus å®˜æ–¹æ–‡æ¡£](https://milvus.io/docs)
- [CLIP æ¨¡å‹è¯¦è§£](https://github.com/openai/CLIP)
- [å‘é‡æ•°æ®åº“åŸç†](https://zilliz.com/learn/what-is-vector-database)
- [å¤šæ¨¡æ€ AI åº”ç”¨](https://zilliz.com/blog)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Request æ¥æ”¹è¿›è¿™ä¸ªç¤ºä¾‹ï¼

## ï¿½ï¿½ è®¸å¯è¯

MIT License 