# Milvus å›¾æ–‡æœç´¢å®æˆ˜æ•™ç¨‹

## ğŸ¯ ç›®æ ‡
æ„å»ºä¸€ä¸ªæ”¯æŒæ–‡æœ¬æœç´¢å›¾åƒã€å›¾åƒæœç´¢æ–‡æœ¬çš„å¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿã€‚

## ğŸ“¦ ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install pymilvus transformers torch pillow requests numpy

# å¯åŠ¨ Milvus (ä½¿ç”¨ docker-compose)
docker-compose up -d
```

## ğŸ”§ æ ¸å¿ƒæ­¥éª¤

### 1. è¿æ¥æ•°æ®åº“
```python
from pymilvus import MilvusClient
client = MilvusClient("http://localhost:19530")
```

### 2. åˆ›å»ºé›†åˆ
```python
# å®šä¹‰å¤šæ¨¡æ€é›†åˆç»“æ„
collection_schema = {
    "fields": [
        {"name": "id", "type": "INT64", "is_primary": True},
        {"name": "image_path", "type": "VARCHAR", "max_length": 500},
        {"name": "description", "type": "VARCHAR", "max_length": 1000}, 
        {"name": "image_vector", "type": "FLOAT_VECTOR", "dim": 512},
        {"name": "text_vector", "type": "FLOAT_VECTOR", "dim": 512}
    ]
}
```

### 3. å‘é‡åŒ–å¤„ç†
```python
from transformers import CLIPProcessor, CLIPModel

# åŠ è½½ CLIP æ¨¡å‹
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

def encode_image(image_path):
    image = Image.open(image_path)
    inputs = processor(images=image, return_tensors="pt")
    features = model.get_image_features(**inputs)
    return features.detach().numpy().flatten()

def encode_text(text):
    inputs = processor(text=text, return_tensors="pt")  
    features = model.get_text_features(**inputs)
    return features.detach().numpy().flatten()
```

### 4. æ•°æ®æ’å…¥
```python
# å‡†å¤‡æ•°æ®
data = []
for image_path, description in image_text_pairs:
    data.append({
        "image_path": image_path,
        "description": description,
        "image_vector": encode_image(image_path).tolist(),
        "text_vector": encode_text(description).tolist()
    })

# æ’å…¥å‘é‡æ•°æ®åº“
client.insert("multimodal_collection", data)
```

### 5. åˆ›å»ºç´¢å¼•
```python
# ä¸ºå‘é‡å­—æ®µåˆ›å»ºé«˜æ€§èƒ½ç´¢å¼•
index_params = {
    "index_type": "HNSW",
    "metric_type": "COSINE",
    "params": {"M": 16, "efConstruction": 200}
}

client.create_index("multimodal_collection", "image_vector", index_params)
client.create_index("multimodal_collection", "text_vector", index_params)
```

### 6. è·¨æ¨¡æ€æœç´¢
```python
# æ–‡æœ¬æœç´¢å›¾åƒ
def text_to_image_search(query_text, top_k=5):
    query_vector = encode_text(query_text)
    results = client.search(
        collection_name="multimodal_collection",
        data=[query_vector.tolist()],
        anns_field="image_vector",  # ç”¨æ–‡æœ¬å‘é‡æœç´¢å›¾åƒå‘é‡
        limit=top_k
    )
    return results

# å›¾åƒæœç´¢æ–‡æœ¬  
def image_to_text_search(query_image, top_k=5):
    query_vector = encode_image(query_image)
    results = client.search(
        collection_name="multimodal_collection", 
        data=[query_vector.tolist()],
        anns_field="text_vector",   # ç”¨å›¾åƒå‘é‡æœç´¢æ–‡æœ¬å‘é‡
        limit=top_k
    )
    return results
```

## ğŸš€ è¿è¡Œç¤ºä¾‹

```bash
# è¿è¡Œç®€å•ç¤ºä¾‹
python simple_example.py

# è¿è¡Œå®Œæ•´ç¤ºä¾‹  
python multimodal_search_example.py
```

## ğŸ’¡ åº”ç”¨åœºæ™¯

1. **ç”µå•†æœç´¢**: ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡æœç´¢ç›¸ä¼¼å•†å“
2. **å†…å®¹ç®¡ç†**: æ ¹æ®æ–‡æœ¬æœç´¢ç›¸å…³å›¾ç‰‡ç´ æ
3. **æ™ºèƒ½ç›¸å†Œ**: è¯­ä¹‰åŒ–æœç´¢ä¸ªäººç…§ç‰‡
4. **æ¨èç³»ç»Ÿ**: è·¨æ¨¡æ€å†…å®¹æ¨è

## ğŸ›ï¸ æ€§èƒ½ä¼˜åŒ–

- é€‰æ‹©åˆé€‚çš„å‘é‡ç»´åº¦ (384/512/768)
- è°ƒæ•´ç´¢å¼•å‚æ•°å¹³è¡¡æ€§èƒ½å’Œç²¾åº¦
- ä½¿ç”¨ GPU åŠ é€Ÿå‘é‡è®¡ç®—
- æ‰¹é‡å¤„ç†æå‡æ’å…¥æ•ˆç‡ 